{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation..\n",
      "--------------------------------------\n",
      "\n",
      "Loading pipeline\n",
      "pipeline loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import dill\n",
    "\n",
    "classicML = True\n",
    "preprocessing = True\n",
    "\n",
    "print(\"\\nStarting evaluation..\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "print(\"Loading pipeline\")\n",
    "pipeline = joblib.load(\"pipeline.pkl\")\n",
    "print(\"pipeline loaded\")\n",
    "\n",
    "if classicML == False:\n",
    "    from keras.models import load_model\n",
    "    print(\"Loading model\")\n",
    "    classifier = load_model(\"model.h5\")\n",
    "    print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "X = test_df.iloc[:,1:]\n",
    "y = test_df.iloc[:,0]\n",
    "\n",
    "print(\"Running predictions\")\n",
    "if classicML == False:\n",
    "    if preprocessing:\n",
    "        X = pipeline.transform(X)\n",
    "    y_pred = classifier.predict_classes(X)\n",
    "    y_proba = classifier.predict_proba(X)\n",
    "else:\n",
    "    y_pred = pipeline.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(y_true, y_pred, verbose = True):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "   \n",
    "    #Calculate metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    #auc_global = roc_auc_score(y, y_pred, average = 'micro')\n",
    "    #auc_per_class = roc_auc_score(y, y_pred)\n",
    "    prfs_global = precision_recall_fscore_support(y_true, y_pred, average = 'micro')\n",
    "    \n",
    "    pr_score_global = precision_score(y_true, y_pred, average = 'micro')\n",
    "    pr_score_per_class = precision_score(y_true, y_pred, average = None)\n",
    "    recall_score_global = recall_score(y_true, y_pred, average = 'micro')\n",
    "    recall_score_per_class = recall_score(y_true, y_pred, average = None)\n",
    "    f1_score_global = f1_score(y_true, y_pred, average = 'micro')\n",
    "    f1_score_per_class = f1_score(y_true, y_pred, average = None)\n",
    "    \n",
    "    prfs_per_class = precision_recall_fscore_support(y_true, y_pred)\n",
    "    testSize = len(test_df.loc[:,'pixel0'])\n",
    "    \n",
    "    #Get trainlog-results\n",
    "    logname = \"train_log.xlsx\"\n",
    "    train_log_df = pd.read_excel(logname)\n",
    "    train_time = train_log_df.loc['Time'].values\n",
    "    trainSize = train_log_df.loc['TrainSize'].values\n",
    "    \n",
    "    if verbose:\n",
    "        #Visualize Global(Averaged if mutliclass) results\n",
    "        print(\"Test results\\n-------------------------------------------------------------\\n\")\n",
    "        print(\"Global statistics\")\n",
    "        print(\"Accuracy: {0}%\".format(accuracy))\n",
    "        print(\"Matthews correlation coefficient: {0}\".format(mcc))\n",
    "        print(\"Precision: {0}\".format(pr_score_global))\n",
    "        print(\"Recall: {0}\".format(recall_score_global))\n",
    "        print(\"F1-Score: {0}\".format(f1_score_global))\n",
    "\n",
    "        #Visualize and save per-class results\n",
    "        print(\"\\nPer-Class statistics\")\n",
    "        print(\"Precision: {0}\".format(pr_score_per_class))\n",
    "        print(\"Recall: {0}\".format(recall_score_per_class))\n",
    "        print(\"F1-Score: {0}\".format(f1_score_per_class))\n",
    "        print(\"\\n\")\n",
    "        print(\"\\nConfusion matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "    results = {\"Train_size\":trainSize[0], \"Train_time\":train_time[0], \"Test_size\":testSize, \"Accuracy\":accuracy, \n",
    "                   \"MCC\":mcc, \"Precision_global:\":pr_score_global, \"Recall_global:\":recall_score_global,\n",
    "                   \"F1_global:\":f1_score_global\n",
    "              } \n",
    "    \n",
    "    #Now update with per-class scores.\n",
    "    for classNr in range(0, cm.shape[0], 1):\n",
    "        base = \"_class_\" + str(classNr)\n",
    "        pr = \"Precision\" + base \n",
    "        re = \"Recall\" + base \n",
    "        f1 = \"F1\" + base\n",
    "        results.update({pr:pr_score_per_class[classNr], re:recall_score_per_class[classNr], f1:f1_score_per_class[classNr]})\n",
    "        \n",
    "    return results\n",
    "\n",
    "def results_to_excel(Results, Filename):\n",
    "    from pathlib import Path\n",
    "    #results_df = pd.DataFrame.from_dict(Results, orient = 'columns')\n",
    "    results_df = pd.DataFrame([Results])\n",
    "    \n",
    "    file = Path(Filename)\n",
    "    if file.exists():\n",
    "        print(\"Found previous averaged metric results, appending new results.\")\n",
    "        oldresults = pd.read_excel(Filename)\n",
    "        results_df = results_df.append(oldresults, ignore_index = True)\n",
    "        results_df = results_df.sort_values(\"Train_size\")\n",
    "        print(results_df)\n",
    "    else:\n",
    "        print(\"Found no previous results, making a new file with results.\")\n",
    "    results_df.to_excel(Filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "foundResults = False\n",
    "\n",
    "from pathlib import Path\n",
    "if Path('results.json').exists():\n",
    "    with open('results.json', 'r') as f:\n",
    "        previousResults = json.load(f)\n",
    "    foundResults = True\n",
    "else:\n",
    "    print(\"Warning, did not find any previous results!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareResults(Results, PreviousResults):\n",
    "    print(\"Comparing results\")\n",
    "    if(Results['Accuracy'] >= PreviousResults['Accuracy']):\n",
    "        print(\"\\nModel passed the test.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Model did not pass the test.\")\n",
    "        return False\n",
    "    \n",
    "def modelPassed(Results, cm):\n",
    "    #Save the results as the current model\n",
    "    filename = \"metric_results.xlsx\"\n",
    "    \n",
    "    Results.update({\"Confusion_matrix:\":cm.tolist()})\n",
    "    results_to_excel(Results, filename)\n",
    "    print(\"----------------\")\n",
    "    print(Results)\n",
    "    \n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(Results, f, ensure_ascii=False, sort_keys=False, indent = 4) #Dump confusion_matrix\n",
    "    #Save the results in version control\n",
    "    with open('./Versions/Pipeline_' + version +'_results.json', 'w') as f:\n",
    "        json.dump(Results, f, ensure_ascii=False, sort_keys=True, indent = 4) #Dump confusion_matrix\n",
    "\n",
    "\n",
    "def modelFailed(Results, cm):  \n",
    "    print(\"Saving results..\")\n",
    "    Results.update({\"Confusion_matrix:\":cm.tolist()})\n",
    "    filename = \"./Versions/Failed/metric_results_failed.xlsx\"\n",
    "    results_to_excel(Results, filename)\n",
    "\n",
    "    \n",
    "    with open('./Versions/Failed/Pipeline_' + version +'_results.json', 'w') as f:\n",
    "        json.dump(Results, f, ensure_ascii=False, sort_keys=True, indent = 4)\n",
    "    print(\"Results saved.\")\n",
    "    import sys\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results\n",
      "-------------------------------------------------------------\n",
      "\n",
      "Global statistics\n",
      "Accuracy: 0.99%\n",
      "Matthews correlation coefficient: 0.9888856765432191\n",
      "Precision: 0.99\n",
      "Recall: 0.99\n",
      "F1-Score: 0.99\n",
      "\n",
      "Per-Class statistics\n",
      "Precision: [ 0.99067599  0.984375    0.98748436  0.99201824  0.99378882  0.99057873\n",
      "  0.9939976   0.98987627  0.98745295  0.99046484]\n",
      "Recall: [ 0.99765258  0.99473684  0.98501873  0.98639456  0.9864365   0.98527443\n",
      "  0.99638989  0.99547511  0.98129676  0.98928571]\n",
      "F1-Score: [ 0.99415205  0.9895288   0.98625     0.98919841  0.99009901  0.98791946\n",
      "  0.99519231  0.99266779  0.98436523  0.98987493]\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[850   0   0   0   0   0   1   0   1   0]\n",
      " [  0 945   0   1   0   0   0   1   3   0]\n",
      " [  2   0 789   2   0   1   0   4   2   1]\n",
      " [  1   3   3 870   0   3   0   1   0   1]\n",
      " [  0   2   1   0 800   0   2   0   2   4]\n",
      " [  2   2   0   2   1 736   1   0   1   2]\n",
      " [  1   0   1   0   1   0 828   0   0   0]\n",
      " [  0   0   3   0   0   1   0 880   0   0]\n",
      " [  1   8   1   2   1   1   1   0 787   0]\n",
      " [  1   0   1   0   2   1   0   3   1 831]]\n",
      "Comparing results\n",
      "\n",
      "Model passed the test.\n",
      "Found previous averaged metric results, appending new results.\n",
      "   Accuracy                                  Confusion_matrix:  F1_class_0  \\\n",
      "0      0.99  [[850, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 945, 0,...    0.994152   \n",
      "1      0.99  [[850, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 945, 0,...    0.994152   \n",
      "\n",
      "   F1_class_1  F1_class_2  F1_class_3  F1_class_4  F1_class_5  F1_class_6  \\\n",
      "0    0.989529     0.98625    0.989198    0.990099    0.987919    0.995192   \n",
      "1    0.989529     0.98625    0.989198    0.990099    0.987919    0.995192   \n",
      "\n",
      "   F1_class_7     ...      Recall_class_4  Recall_class_5  Recall_class_6  \\\n",
      "0    0.992668     ...            0.986436        0.985274         0.99639   \n",
      "1    0.992668     ...            0.986436        0.985274         0.99639   \n",
      "\n",
      "   Recall_class_7  Recall_class_8  Recall_class_9  Recall_global:  Test_size  \\\n",
      "0        0.995475        0.981297        0.989286            0.99       8400   \n",
      "1        0.995475        0.981297        0.989286            0.99       8400   \n",
      "\n",
      "   Train_size  Train_time  \n",
      "0      7560.0   33.675168  \n",
      "1      7560.0   33.675168  \n",
      "\n",
      "[2 rows x 39 columns]\n",
      "----------------\n",
      "{'Train_size': 7560.0, 'Train_time': 33.675167999999999, 'Test_size': 8400, 'Accuracy': 0.98999999999999999, 'MCC': 0.98888567654321913, 'Precision_global:': 0.98999999999999999, 'Recall_global:': 0.98999999999999999, 'F1_global:': 0.98999999999999999, 'Precision_class_0': 0.99067599067599066, 'Recall_class_0': 0.99765258215962438, 'F1_class_0': 0.99415204678362568, 'Precision_class_1': 0.984375, 'Recall_class_1': 0.99473684210526314, 'F1_class_1': 0.98952879581151831, 'Precision_class_2': 0.98748435544430535, 'Recall_class_2': 0.98501872659176026, 'F1_class_2': 0.98624999999999996, 'Precision_class_3': 0.99201824401368299, 'Recall_class_3': 0.98639455782312924, 'F1_class_3': 0.98919840818646965, 'Precision_class_4': 0.99378881987577639, 'Recall_class_4': 0.98643649815043155, 'F1_class_4': 0.99009900990099009, 'Precision_class_5': 0.99057873485868098, 'Recall_class_5': 0.98527443105756363, 'F1_class_5': 0.98791946308724832, 'Precision_class_6': 0.99399759903961582, 'Recall_class_6': 0.99638989169675085, 'F1_class_6': 0.9951923076923076, 'Precision_class_7': 0.98987626546681662, 'Recall_class_7': 0.99547511312217196, 'F1_class_7': 0.99266779469825139, 'Precision_class_8': 0.98745294855708909, 'Recall_class_8': 0.98129675810473815, 'F1_class_8': 0.98436522826766737, 'Precision_class_9': 0.99046483909415972, 'Recall_class_9': 0.98928571428571432, 'F1_class_9': 0.98987492555092316, 'Confusion_matrix:': [[850, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 945, 0, 1, 0, 0, 0, 1, 3, 0], [2, 0, 789, 2, 0, 1, 0, 4, 2, 1], [1, 3, 3, 870, 0, 3, 0, 1, 0, 1], [0, 2, 1, 0, 800, 0, 2, 0, 2, 4], [2, 2, 0, 2, 1, 736, 1, 0, 1, 2], [1, 0, 1, 0, 1, 0, 828, 0, 0, 0], [0, 0, 3, 0, 0, 1, 0, 880, 0, 0], [1, 8, 1, 2, 1, 1, 1, 0, 787, 0], [1, 0, 1, 0, 2, 1, 0, 3, 1, 831]]}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "version = str(datetime.datetime.now())\n",
    "\n",
    "results = getResults(y, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "if foundResults:\n",
    "    if compareResults(results, previousResults):\n",
    "        modelPassed(results, cm)\n",
    "    else:\n",
    "        modelFailed(results, cm)\n",
    "else:\n",
    "    modelPassed(results, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

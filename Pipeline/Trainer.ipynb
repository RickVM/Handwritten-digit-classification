{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting model training..\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\\nStarting model training..\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "train_df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search code\n",
    "\n",
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "\n",
    "from scipy.stats import randint\n",
    "import scipy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1), copy=True)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "params = {'max_depth': randint(1, 10) ,\n",
    "          'learning_rate': scipy.stats.expon(scale=0.5),\n",
    "          'n_estimators': randint(1, 10),\n",
    "          'gamma': scipy.stats.expon(scale=1)\n",
    "           }\n",
    "\n",
    "\n",
    "import datetime\n",
    "print(\"\\n\\nStarting parameter search..\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "tstart = datetime.datetime.now()\n",
    "optimizer = RandomizedSearchCV(XGBClassifier(objective = 'multi:softmax'), params, n_iter = 25)\n",
    "optimizer.fit(X, y)\n",
    "tstop = datetime.datetime.now()\n",
    "tdelta = tstop - tstart\n",
    "print(\"Finished training.\")\n",
    "print(\"Training duration in (Days/Hours/Seconds/Milliseconds): {0}\".format(tdelta)) \n",
    "print(optimizer.score()) #0.91130952380952379\n",
    "print(optimizer.best_params_) #{'gamma': 0.90390078036156596,\n",
    "                              #'learning_rate': 0.37483528867120858,\n",
    "                              #'max_depth': 9,\n",
    "                              #'n_estimators': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating a pipeline we can run one line of code to pre-process our data and train our model.\n",
    "Later on this pipeline will also enable us to only need 1 line of code to pre-process and make predictions on new data.\n",
    "Hence the code will be a lot cleaner.\n",
    "\n",
    "Pipeline.steps can be called to view the all the components and parameters that make up the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# XGBoost Code\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#Default settings - Now deprecated with optimized settings. \n",
    "#pipeline = Pipeline(steps=[('minmaxscaler', MinMaxScaler(feature_range=(0, 1), copy=True)),\n",
    "#                            ('XgbClassifier', XGBClassifier(objective = 'multi:softmax'))\n",
    "#                           ])\n",
    "\n",
    "#Based upon random search results.\n",
    "#Accuracy score was 91% on 20% of the training data.\n",
    "\n",
    "pipeline = Pipeline(steps=[('minmaxscaler', MinMaxScaler(feature_range=(0, 1), copy=True)),\n",
    "                           ('XgbClassifier', XGBClassifier(objective = 'multi:softmax',\n",
    "                                                           gamma = 0.9,\n",
    "                                                           learning_rate = 0.375,\n",
    "                                                           max_depth = 9,\n",
    "                                                           n_estimators = 9))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeplearning Code\n",
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #units = (Input+Output)/2\n",
    "    model.add(Dense(units = (int(round(len(X.columns)+9)/2)), activation = 'relu', input_dim= len(X.columns))) \n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "    return model\n",
    "    \n",
    "pipeline = Pipeline(steps=[('minmaxscaler', MinMaxScaler(feature_range=(0, 1), copy=True)),\n",
    "                            ('DeepNet', KerasClassifier(build_fn = create_model, epochs = 150, batch_size = 10))\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2100/2100 [==============================] - 1s 685us/step - loss: 0.7467\n",
      "Epoch 2/150\n",
      "2100/2100 [==============================] - 1s 481us/step - loss: 0.3250\n",
      "Epoch 3/150\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 0.2210\n",
      "Epoch 4/150\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 0.1456\n",
      "Epoch 5/150\n",
      "2100/2100 [==============================] - 1s 499us/step - loss: 0.1037\n",
      "Epoch 6/150\n",
      "2100/2100 [==============================] - 1s 497us/step - loss: 0.0662\n",
      "Epoch 7/150\n",
      "2100/2100 [==============================] - 1s 503us/step - loss: 0.0479\n",
      "Epoch 8/150\n",
      "2100/2100 [==============================] - 1s 488us/step - loss: 0.0343\n",
      "Epoch 9/150\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 0.0253\n",
      "Epoch 10/150\n",
      "2100/2100 [==============================] - 1s 552us/step - loss: 0.0202\n",
      "Epoch 11/150\n",
      "2100/2100 [==============================] - 1s 517us/step - loss: 0.0154\n",
      "Epoch 12/150\n",
      "2100/2100 [==============================] - 1s 491us/step - loss: 0.0136\n",
      "Epoch 13/150\n",
      "2100/2100 [==============================] - 1s 491us/step - loss: 0.0116\n",
      "Epoch 14/150\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 0.0101\n",
      "Epoch 15/150\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 0.0090\n",
      "Epoch 16/150\n",
      "2100/2100 [==============================] - 1s 466us/step - loss: 0.0080\n",
      "Epoch 17/150\n",
      "2100/2100 [==============================] - 1s 495us/step - loss: 0.0072\n",
      "Epoch 18/150\n",
      "2100/2100 [==============================] - 1s 467us/step - loss: 0.0067\n",
      "Epoch 19/150\n",
      "2100/2100 [==============================] - 1s 502us/step - loss: 0.0061\n",
      "Epoch 20/150\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 0.0057\n",
      "Epoch 21/150\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 0.0053\n",
      "Epoch 22/150\n",
      "2100/2100 [==============================] - 1s 491us/step - loss: 0.0049\n",
      "Epoch 23/150\n",
      "2100/2100 [==============================] - 1s 501us/step - loss: 0.0046\n",
      "Epoch 24/150\n",
      "2100/2100 [==============================] - 1s 519us/step - loss: 0.0044\n",
      "Epoch 25/150\n",
      "2100/2100 [==============================] - 1s 517us/step - loss: 0.0041\n",
      "Epoch 26/150\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 0.0039\n",
      "Epoch 27/150\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 0.0037\n",
      "Epoch 28/150\n",
      "2100/2100 [==============================] - 1s 496us/step - loss: 0.0035\n",
      "Epoch 29/150\n",
      "2100/2100 [==============================] - 1s 508us/step - loss: 0.0033\n",
      "Epoch 30/150\n",
      "2100/2100 [==============================] - 1s 500us/step - loss: 0.0032\n",
      "Epoch 31/150\n",
      "2100/2100 [==============================] - 1s 512us/step - loss: 0.0031\n",
      "Epoch 32/150\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 0.0029\n",
      "Epoch 33/150\n",
      "2100/2100 [==============================] - 1s 495us/step - loss: 0.0028\n",
      "Epoch 34/150\n",
      "2100/2100 [==============================] - 1s 481us/step - loss: 0.0027\n",
      "Epoch 35/150\n",
      "2100/2100 [==============================] - 1s 504us/step - loss: 0.0026\n",
      "Epoch 36/150\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 0.0025\n",
      "Epoch 37/150\n",
      "2100/2100 [==============================] - 1s 497us/step - loss: 0.0024\n",
      "Epoch 38/150\n",
      "2100/2100 [==============================] - 1s 523us/step - loss: 0.0023\n",
      "Epoch 39/150\n",
      "2100/2100 [==============================] - 1s 537us/step - loss: 0.0022\n",
      "Epoch 40/150\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 0.0022\n",
      "Epoch 41/150\n",
      "2100/2100 [==============================] - 1s 499us/step - loss: 0.0021\n",
      "Epoch 42/150\n",
      "2100/2100 [==============================] - 1s 538us/step - loss: 0.0020\n",
      "Epoch 43/150\n",
      "2100/2100 [==============================] - 1s 524us/step - loss: 0.0020\n",
      "Epoch 44/150\n",
      "2100/2100 [==============================] - 1s 545us/step - loss: 0.0019\n",
      "Epoch 45/150\n",
      "2100/2100 [==============================] - 1s 515us/step - loss: 0.0019\n",
      "Epoch 46/150\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 0.0018\n",
      "Epoch 47/150\n",
      "2100/2100 [==============================] - 1s 502us/step - loss: 0.0018\n",
      "Epoch 48/150\n",
      "2100/2100 [==============================] - 1s 507us/step - loss: 0.0017\n",
      "Epoch 49/150\n",
      "2100/2100 [==============================] - 1s 513us/step - loss: 0.0017\n",
      "Epoch 50/150\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 0.0016\n",
      "Epoch 51/150\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 0.0016\n",
      "Epoch 52/150\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 0.0016\n",
      "Epoch 53/150\n",
      "2100/2100 [==============================] - 1s 509us/step - loss: 0.0015\n",
      "Epoch 54/150\n",
      "2100/2100 [==============================] - 1s 506us/step - loss: 0.0015\n",
      "Epoch 55/150\n",
      "2100/2100 [==============================] - 1s 541us/step - loss: 0.0015\n",
      "Epoch 56/150\n",
      "2100/2100 [==============================] - 1s 518us/step - loss: 0.0014\n",
      "Epoch 57/150\n",
      "2100/2100 [==============================] - 1s 509us/step - loss: 0.0014\n",
      "Epoch 58/150\n",
      "2100/2100 [==============================] - 1s 562us/step - loss: 0.0014\n",
      "Epoch 59/150\n",
      "2100/2100 [==============================] - 1s 516us/step - loss: 0.0013\n",
      "Epoch 60/150\n",
      "2100/2100 [==============================] - 1s 534us/step - loss: 0.0013\n",
      "Epoch 61/150\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 0.0013\n",
      "Epoch 62/150\n",
      "2100/2100 [==============================] - 1s 529us/step - loss: 0.0013\n",
      "Epoch 63/150\n",
      "2100/2100 [==============================] - 1s 518us/step - loss: 0.0012\n",
      "Epoch 64/150\n",
      "2100/2100 [==============================] - 1s 519us/step - loss: 0.0012\n",
      "Epoch 65/150\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 0.0012\n",
      "Epoch 66/150\n",
      "2100/2100 [==============================] - 1s 498us/step - loss: 0.0012\n",
      "Epoch 67/150\n",
      "2100/2100 [==============================] - 1s 552us/step - loss: 0.0011\n",
      "Epoch 68/150\n",
      "2100/2100 [==============================] - 1s 516us/step - loss: 0.0011\n",
      "Epoch 69/150\n",
      "2100/2100 [==============================] - 1s 501us/step - loss: 0.0011\n",
      "Epoch 70/150\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 0.0011\n",
      "Epoch 71/150\n",
      "2100/2100 [==============================] - 1s 519us/step - loss: 0.0011\n",
      "Epoch 72/150\n",
      "2100/2100 [==============================] - 1s 523us/step - loss: 0.0010\n",
      "Epoch 73/150\n",
      "2100/2100 [==============================] - 1s 501us/step - loss: 0.0010\n",
      "Epoch 74/150\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 0.0010\n",
      "Epoch 75/150\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 9.9167e-04\n",
      "Epoch 76/150\n",
      "2100/2100 [==============================] - 1s 525us/step - loss: 9.7799e-04\n",
      "Epoch 77/150\n",
      "2100/2100 [==============================] - 1s 540us/step - loss: 9.6062e-04\n",
      "Epoch 78/150\n",
      "2100/2100 [==============================] - 1s 559us/step - loss: 9.4637e-04\n",
      "Epoch 79/150\n",
      "2100/2100 [==============================] - 1s 537us/step - loss: 9.3121e-04\n",
      "Epoch 80/150\n",
      "2100/2100 [==============================] - 1s 549us/step - loss: 9.1702e-04\n",
      "Epoch 81/150\n",
      "2100/2100 [==============================] - 1s 537us/step - loss: 9.0373e-04\n",
      "Epoch 82/150\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 8.9161e-04\n",
      "Epoch 83/150\n",
      "2100/2100 [==============================] - 1s 517us/step - loss: 8.7777e-04\n",
      "Epoch 84/150\n",
      "2100/2100 [==============================] - 1s 548us/step - loss: 8.6238e-04\n",
      "Epoch 85/150\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 8.5205e-04\n",
      "Epoch 86/150\n",
      "2100/2100 [==============================] - 1s 548us/step - loss: 8.4053e-04\n",
      "Epoch 87/150\n",
      "2100/2100 [==============================] - 1s 476us/step - loss: 8.2715e-04\n",
      "Epoch 88/150\n",
      "2100/2100 [==============================] - 1s 549us/step - loss: 8.1765e-04\n",
      "Epoch 89/150\n",
      "2100/2100 [==============================] - 1s 513us/step - loss: 8.0639e-04\n",
      "Epoch 90/150\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 7.9722e-04\n",
      "Epoch 91/150\n",
      "2100/2100 [==============================] - 1s 502us/step - loss: 7.8383e-04\n",
      "Epoch 92/150\n",
      "2100/2100 [==============================] - 1s 564us/step - loss: 7.7465e-04\n",
      "Epoch 93/150\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 7.6470e-04\n",
      "Epoch 94/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 476us/step - loss: 7.5442e-04\n",
      "Epoch 95/150\n",
      "2100/2100 [==============================] - 1s 474us/step - loss: 7.4586e-04\n",
      "Epoch 96/150\n",
      "2100/2100 [==============================] - 1s 631us/step - loss: 7.3505e-04\n",
      "Epoch 97/150\n",
      "2100/2100 [==============================] - 1s 609us/step - loss: 7.2724e-04\n",
      "Epoch 98/150\n",
      "2100/2100 [==============================] - 1s 475us/step - loss: 7.1750e-04\n",
      "Epoch 99/150\n",
      "2100/2100 [==============================] - 1s 471us/step - loss: 7.0901e-04\n",
      "Epoch 100/150\n",
      "2100/2100 [==============================] - 1s 483us/step - loss: 7.0059e-04\n",
      "Epoch 101/150\n",
      "2100/2100 [==============================] - 1s 540us/step - loss: 6.9323e-04\n",
      "Epoch 102/150\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 6.8313e-04\n",
      "Epoch 103/150\n",
      "2100/2100 [==============================] - 1s 472us/step - loss: 6.7655e-04\n",
      "Epoch 104/150\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 6.6908e-04\n",
      "Epoch 105/150\n",
      "2100/2100 [==============================] - 1s 448us/step - loss: 6.6106e-04\n",
      "Epoch 106/150\n",
      "2100/2100 [==============================] - 1s 484us/step - loss: 6.5367e-04\n",
      "Epoch 107/150\n",
      "2100/2100 [==============================] - 1s 472us/step - loss: 6.4599e-04\n",
      "Epoch 108/150\n",
      "2100/2100 [==============================] - 1s 468us/step - loss: 6.3848e-04\n",
      "Epoch 109/150\n",
      "2100/2100 [==============================] - 1s 496us/step - loss: 6.3228e-04\n",
      "Epoch 110/150\n",
      "2100/2100 [==============================] - 1s 480us/step - loss: 6.2518e-04\n",
      "Epoch 111/150\n",
      "2100/2100 [==============================] - 1s 498us/step - loss: 6.1870e-04\n",
      "Epoch 112/150\n",
      "2100/2100 [==============================] - 1s 471us/step - loss: 6.1171e-04\n",
      "Epoch 113/150\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 6.0504e-04\n",
      "Epoch 114/150\n",
      "2100/2100 [==============================] - 2s 776us/step - loss: 5.9966e-04\n",
      "Epoch 115/150\n",
      "2100/2100 [==============================] - 1s 676us/step - loss: 5.9310e-04\n",
      "Epoch 116/150\n",
      "2100/2100 [==============================] - 1s 617us/step - loss: 5.8703e-04\n",
      "Epoch 117/150\n",
      "2100/2100 [==============================] - 1s 466us/step - loss: 5.8064e-04\n",
      "Epoch 118/150\n",
      "2100/2100 [==============================] - 1s 490us/step - loss: 5.7633e-04\n",
      "Epoch 119/150\n",
      "2100/2100 [==============================] - 1s 479us/step - loss: 5.6935e-04\n",
      "Epoch 120/150\n",
      "2100/2100 [==============================] - 1s 466us/step - loss: 5.6376e-04\n",
      "Epoch 121/150\n",
      "2100/2100 [==============================] - 1s 468us/step - loss: 5.5822e-04\n",
      "Epoch 122/150\n",
      "2100/2100 [==============================] - 1s 476us/step - loss: 5.5250e-04\n",
      "Epoch 123/150\n",
      "2100/2100 [==============================] - 1s 473us/step - loss: 5.4754e-04\n",
      "Epoch 124/150\n",
      "2100/2100 [==============================] - 1s 474us/step - loss: 5.4187e-04\n",
      "Epoch 125/150\n",
      "2100/2100 [==============================] - 1s 481us/step - loss: 5.3679e-04\n",
      "Epoch 126/150\n",
      "2100/2100 [==============================] - 1s 468us/step - loss: 5.3192e-04\n",
      "Epoch 127/150\n",
      "2100/2100 [==============================] - 1s 525us/step - loss: 5.2642e-04\n",
      "Epoch 128/150\n",
      "2100/2100 [==============================] - 1s 508us/step - loss: 5.2233e-04\n",
      "Epoch 129/150\n",
      "2100/2100 [==============================] - 1s 485us/step - loss: 5.1743e-04\n",
      "Epoch 130/150\n",
      "2100/2100 [==============================] - 1s 473us/step - loss: 5.1305e-04\n",
      "Epoch 131/150\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 5.0758e-04\n",
      "Epoch 132/150\n",
      "2100/2100 [==============================] - 1s 512us/step - loss: 5.0350e-04\n",
      "Epoch 133/150\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 4.9897e-04\n",
      "Epoch 134/150\n",
      "2100/2100 [==============================] - 1s 561us/step - loss: 4.9456e-04\n",
      "Epoch 135/150\n",
      "2100/2100 [==============================] - 1s 565us/step - loss: 4.9032e-04\n",
      "Epoch 136/150\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 4.8632e-04\n",
      "Epoch 137/150\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 4.8193e-04\n",
      "Epoch 138/150\n",
      "2100/2100 [==============================] - 1s 546us/step - loss: 4.7799e-04\n",
      "Epoch 139/150\n",
      "2100/2100 [==============================] - 1s 536us/step - loss: 4.7470e-04\n",
      "Epoch 140/150\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 4.6911e-04\n",
      "Epoch 141/150\n",
      "2100/2100 [==============================] - 1s 490us/step - loss: 4.6617e-04\n",
      "Epoch 142/150\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 4.6217e-04\n",
      "Epoch 143/150\n",
      "2100/2100 [==============================] - 1s 518us/step - loss: 4.5740e-04\n",
      "Epoch 144/150\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 4.5446e-04\n",
      "Epoch 145/150\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 4.5109e-04\n",
      "Epoch 146/150\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 4.4667e-04\n",
      "Epoch 147/150\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 4.4307e-04\n",
      "Epoch 148/150\n",
      "2100/2100 [==============================] - 1s 505us/step - loss: 4.3973e-04\n",
      "Epoch 149/150\n",
      "2100/2100 [==============================] - 1s 533us/step - loss: 4.3639e-04\n",
      "Epoch 150/150\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 4.3322e-04\n",
      "Finished training.\n",
      "Training duration in (/Hours/Minutes/Seconds/Milliseconds): 0:02:43.111743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('DeepNet', <keras.wrappers.scikit_learn.KerasClassifier at 0x7fecf6adacc0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "tstart = datetime.datetime.now()\n",
    "pipeline = pipeline.fit(X, y)\n",
    "tstop = datetime.datetime.now()\n",
    "tdelta = tstop - tstart\n",
    "print(\"Finished training.\")\n",
    "print(\"Training duration in (/Hours/Minutes/Seconds/Milliseconds): {0}\".format(tdelta))\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont evaluate the model as this is up to the watcher, thus we save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been saved as pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"pipeline.pkl\")\n",
    "print(\"Pipeline has been saved as pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results\n",
      "-------------------------------------------------------------\n",
      "\n",
      "Accuracy: 0.9235714285714286%\n",
      "\n",
      "Confusion matrix:\n",
      "[[823   0   5   2   3   6   7   1   5   0]\n",
      " [  0 932   2   4   3   4   0   1   2   2]\n",
      " [  7   0 739  12  14   3  14   4   4   4]\n",
      " [  2  11  17 786   0  32   5  11   9   9]\n",
      " [  1   5   5   0 755   0  13   1   6  25]\n",
      " [ 10   5   6  15  10 665  11   2  17   6]\n",
      " [  7   4   5   0   6  18 785   1   4   1]\n",
      " [  2   2  21   3  10   6   0 817   2  21]\n",
      " [  5  28  14  16   3  17   5   2 708   4]\n",
      " [ 14   4   8  10  27   9   0  16   4 748]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
